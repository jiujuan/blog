## 前言

在互联网软件开发中，高并发架构代表着用户多，大流量。比如淘宝、天猫购物网站、京东购物网站、抖音短视频等产品。

还有天猫双十一活动，京东168抢购活动，这2个购物活动都是瞬时高并发系统。它们都是业务复杂度高、链路长、数据量大的系统。

这些系统都是在一个很短的时间内会遇到大量请求情况，系统怎么处理这些请求？怎么设计系统？

设计一个高并发架构系统，需要遵循什么原则？需要用到哪些技术？下面做一个解答。

## 高并发架构一般指导原则

设计高并发系统一般指导原则。

### 分而治之

把一个大的应用系统拆分为多个较小的应用系统，把大数据量的库和表拆分为多个库、多个表。这些都是分而治之的思想。

- **垂直拆分**

**应用系统垂直拆分**：可以按照业务功能进行系统拆分，拆分为多个独立的应用。把一个大的应用系统拆分为多个系统，比如大电商系统，可以拆分为商家系统，前台商品和购物系统，后台系统管理系统。

如果再把上面拆分后的应用系统按照微服务进行拆分，进一步拆分为用户、订单、商品等相互独立的微服务，不同团队负责不同服务系统。

**数据库垂直拆分**：将一个包含所有表的单库，拆分为多个库（如订单库、用户库）。订单表归属订单库，用户表归属用户库。

- **水平拆分**

水平拆分一般是按照数据规则进行拆分，解决单库/单表数据量巨大，读写性能瓶颈的问题。

水平分库分表，将海量数据（如亿级订单数据）按照主键ID范围、用户ID取模等规则，分散到多个结构相同的数据库/表中（如 order_0, order_1...）。

### 无状态化

应用服务器不保存会话状态，以便随时水平扩展（Scale Out）。

可以把状态信息（如 session 信息）保存到外置的分布式缓存中（如 redis）。

### 异步化

利用消息队列来削峰填谷，解耦非核心链路业务。如发短信、积分、日志等。

### 冗余备份和副本

保证高可用。

数据冗余 - 主从复制、多副本。

服务冗余 - 多实例部署。

多机房部署、双活/多活部署、异地部署。

### 最终一致性

在高并发场景下，放弃强一致性，追求高可用和最终一致性。

### 缓存

空间换时间。

浏览器缓存，构建本地缓存 （应用内缓存+本机内存缓存+redis）+ 分布式缓存 + CDN的多级缓存体系。

反向代理缓存 Nginx、Varnish。

## 硬件层

高并发系统的是在有限的资源（CPU、内存、磁盘、网络）下，榨取出极致的吞吐量和极低的延迟。

在硬件层面，CPU的多核架构（如SMP、NUMA）、多级缓存（L1/L2/L3）和内存带宽直接影响并行处理能力。

### 多核/多 CPU 与并行

- **CPU核数与线程数**

CPU 内部封装一个或多个物理核，只有 1 个物理核心就是单核 CPU，有多个物理核心就是多核 CPU。

物理 core 核数，硬件线程，还有个超线程技术。高并发依赖多核并行处理。线程数并非越多越好。当线程数超过 CPU 核数时，会发生频繁的上下文切换 (Context Switch)，消耗 CPU 时间片。

> 超线程技术（全名为Hyper-Threading）：

> 这种使 CPU 处理器中的 1 颗物理核，如同 2 颗物理核那样发挥作用，从而提高了系统的整体性能。但肯定不会真的像 2 颗物理核那样，而是借助于某些技术将 1 颗物理核的性能发挥更好而已。

最佳实践：

> CPU 密集型任务，线程数 = CPU 核数 + 1；I/O 密集型任务，可适当增加线程数，但需配合异步 I/O。

指令级并行：

> CPU 通过流水线、分支预测、乱序执行来提高效率。这种乱序执行会导致指令重排，破坏多线程下的内存可见性，需要时会使用使用内存屏障阻止 CPU 的重排序。

CPU 缓存：

> CPU 读取数据先从 L1/L2/L3 缓存读取，速度比内存快几十倍。缓存以 Cache Line (通常 64 字节) 为单位加载。
>
> 使用 缓存行填充 (Padding)，确保高频竞争变量独占 Cache Line（如 Java 的 `@Contended` 注解）。

分支预测：

> CPU 会预测代码执行路径。如果预测失败，流水线需要清空，造成性能损失。
>
> 在高并发热点代码中，减少复杂的 `if-else`，使用无分支编程或查表法。



- **SMP / NUMA / MPP 架构**

**SMP**（Symmetric Multi-Processor）对称多处理器结构， 这中结构指多个 CPU 对称平等，共享相同的物理内存、IO 等资源。因此 SMP结构属于一致存储器访问结构 UMA。早期/小规模服务器常见。

**NUMA**（Non-Uniform Memory Access）非一致存储访问结构，此架构是服务器有多个 CPU 模块，每个 CPU 模块由多个 CPU 组成，每个 CPU 模块具有独立的本地内存，IO 等资源，CPU 模块也称为 Node。因此访问延迟不同，CPU 访问本地内存比访问远端内存快；跨 NUMA node 访问内存慢，是现代多路服务器的主流架构。

高并发场景下，需进行 NUMA 绑定，让进程/线程尽量在分配内存的 CPU 核心上运行，避免跨 CPU 交互导致延迟飙升。这种也叫 CPU 亲和性 - 绑定到特定核或 NUMA node 以减少开销。

**MPP**（Massive Parallel Processing），它是另外一种扩展方式，它由多个 SMP 服务器通过一定的节点网络进行连接，完成相同的任务，可以看作是 SMP 的水平扩展。典型的就是刀片服务器，有的说 MPP 架构很像 MapReduce 模式。

### 内存

**内存带宽 (Memory Bandwidth)**：

高并发下，大量数据拷贝会打满内存带宽。这种情况需减少内存拷贝（零拷贝技术），使用对象池复用内存，避免频繁 GC 分配。

如果内存带宽是所有 CPU 共享，高并发下，内存访问竞争激烈，会导致 CPU 停顿（Stall），即“内存墙”问题。这种情况优化数据结构的紧凑性（如从 `Linked List` 改为 `Array`），利用空间局部性，提高缓存命中率，减少直接访问主内存的次数。

**NUMA架构**，在多路 CPU 服务器上，CPU 访问本地内存快，访问“远程内存”（连接在另一个 CPU 上的内存）慢。如果进程被调度到 CPU 0，但分配的内存属于 CPU 1，访问延迟会增加。开启 NUMA 亲和性，将进程绑定在特定 CPU 节点，并分配该节点的本地内存。

### 网络 网卡

**多队列网卡 (Multi-Queue NIC)**：传统单队列网卡，所有网络中断由一个 CPU 核心处理，成为瓶颈。多队列网卡可将不同连接的中断分配到不同 CPU 核心。利用 RSS (Receive Side Scaling) 技术，根据 IP/Port Hash 将流量分发到不同队列。

还可以配合操作系统的 RPS/RFS，让处理网络数据包的 CPU 核心和执行应用的 CPU 核心保持一致，提高缓存亲和性，避免数据在核心间颠簸。

**智能网卡 (SmartNIC / DPU)**：将 TCP/IP 协议栈、加密、负载均衡卸载到网卡硬件上，释放主机 CPU 资源给业务逻辑。

### 存储 硬盘

**HDD 到 SSD 到 NVMe**：HDD 寻道时间毫秒级，SSD 百微秒级，NVMe 可到十微秒级。

**持久内存 (Optane/PMEM)**：介于内存和磁盘之间，断电不丢失，速度接近内存，用于极致高性能的日志或数据库 WAL。

**NVMe SSD**：高并发系统通常伴随大量日志写入或持久化。机械硬盘 (HDD) 的随机 I/O 性能极差。关注 IOPS (每秒读写次数) 而非吞吐量。NVMe 通过 PCIe 通道直连 CPU，延迟极低。

**其它优化项**：顺序读写远快于随机读写、DMA 技术、零拷贝技术zero copy。

## 操作系统OS

操作系统是硬件资源的管理者，是应用与硬件之间的桥梁。高并发下，如何高效地利用 OS 提供的一些机制高效调度资源。

### 进程/线程调度

**上下文切换开销**：线程切换涉及保存/恢复寄存器、刷新 TLB、调度器算法运行。一次上下文切换耗时通常在微秒级，看似不多，但高并发下每秒成千上万次切换，CPU 就会空转在切换上，而非业务计算。

优化措施：

1、减少线程，使用协程（Goroutine/Fiber）减少 OS 线程数量，由用户态调度，切换成本极低（纳秒级）。还可以使用 Reactor 模型，单线程处理多连接。

2、将关键进程/线程绑定到特定 CPU 核（`taskset` `pthread_setaffinity_np`），避免被 OS 调度到其他核，减少缓存失效。

**调度策略**：Linux 默认调度器为追求公平的 CFS 调度器，但在高并发下，可能需要调整为 `SCHED_FIFO` 或 `SCHED_RR` 实时调度策略，确保核心线程及时获得 CPU。

### 内存管理

**页缓存 (Page Cache)**：Linux 会将磁盘文件缓存在空闲内存中。高并发读多写少场景，利用 Page Cache 可极大提升速度。

**大页内存 (Huge Pages)**：默认页大小 4KB，大页为 2MB/1GB。减少 TLB Miss（页表缺失），提升内存访问速度。

**Swap 交换分区**：高并发系统关闭 Swap。一旦内存不足使用磁盘 Swap（`vm.swappiness=0`），系统性能会下降几个数量级（雪崩）。

**内存分配器**：多线程频繁分配释放内存时，全局锁（如 ptmalloc）会成为瓶颈。使用更高效的内存分配器，如 jemalloc 或 tcmalloc（Google 出品），它们通过线程局部缓存，大幅减少锁竞争。

### I/O 模型

**I/O 多路复用 (Multiplexing)**：select/poll/epoll，epoll 优势，事件回调机制，只通知就绪的 FD，无需遍历所有连接；红黑树 + 就绪链表，高效管理连接状态。

**信号驱动 I/O** 和 **异步 I/O (AIO)**。

**io_uring**：linux AIO 异步IO，基于共享环 (Ring Buffer)，用户态和内核态无需系统调用 (Syscall) 即可提交/获取 I/O 请求，极大减少上下文切换。

### 网络协议栈

Linux 内核的 TCP 参数直接决定网络并发能力，TCP 参数的一些调整：

- 连接复用
  - `tcp_tw_reuse = 1`：允许重用 TIME_WAIT 状态的 socket，解决高并发短连接导致端口耗尽问题。
  - `tcp_keepalive_time`：调整心跳检测，快速清理死连接。

- 队列长度
  - `tcp_max_syn_backlog`：半连接队列长度（SYN Received 状态）。
  - `somaxconn`：全连接队列长度（Established 状态，等待 accept）。

其它 增大 `tcp_rmem / wmem`：扩大读写缓冲区，提升大文件传输效率。

### 中断处理

网卡收到数据包，触发硬中断（HardIRQ），硬中断处理程序要极快完成，然后触发软中断（SoftIRQ）交给内核线程处理协议栈。

如果网络流量巨大，`ksoftirqd` 这个内核线程会占满整个 CPU，导致业务进程无法运行。

优化措施：

查看 `/proc/softirqs` 和 `/proc/interrupts`，监控中断分布。必要时调整中断亲和性（SMP IRQ Affinity），将网卡中断分散到多核，或者使用 Busy Polling 模式，让应用进程主动轮询网卡，减少中断上下文切换。

DPDK 技术：

在用户态直接接管网卡驱动，绕过内核协议栈。完全由用户态轮询网卡，屏蔽内核中断。消除系统调用、中断、内存拷贝开销。转发性能。

DPDK 技术代价是：开发复杂度极高，独占网卡，失去通用网络功能。

## 架构层：分层架构设计

分层架构（Layered Architecture）是互联网系统架构设计中最重要的方法论之一。它将复杂的系统划分为若干个逻辑或物理层次，每一层负责特定的职责，层与层之间通过定义良好的接口协议、消息队列等进行通信。

在[微服务架构学习与思考(03)：微服务总体架构图解](https://www.cnblogs.com/jiujuan/p/13295147.html) 一文中的微服务总体架构图就是一个分层架构图。多数互联网系统架构都可以用这个分层架构或它的变体。

### 分层架构图

技术架构分层架构图，6 层架构设计图：

![架构分层架构图，6层架构图](../images/system-design-high-currrent-arch-layered-architecture-img.png)

下面对各层技术进行简单说明。

### 第一层：客户端层

客户端层是请求的发起端，客户端一般有Web、APP（安卓、IOS）、桌面端、小程序、loT设备、其它终端设备等。

这一层的优化原则：

- 减少请求数、减少数据包大小、优化网络、端侧缓存。

一些优化实践：

1. **资源压缩**：图片 WebP/AVIF 格式，代码压缩 Minify/Gzip。 
2. **合并请求**：HTTP/2 多路复用，GraphQL 减少过度获取。
3. **本地缓存**：利用 LocalStorage/IndexedDB 缓存静态资源
4. **预加载**：根据用户行为预测，提前加载资源

用到的一些网络传输协议：

- HTTP/2, HTTP/3 (QUIC), WebSocket

### 第二层：接入层

接入层是应用系统的入口，负责流量调度、安全等任务。

- DNS：可以实现全球流量调度，实现地理位置级别的负载均衡。

  - 智能 DNS
  - HTTPDNS

- CDN：CDN内容分发网络。将静态资源（图片、视频、CSS文件）缓存到离用户最近的节点，极大提升访问速度，降低源站压力。

- 负载均衡

  - L4 负载均衡：传输层，四层负载均衡，如 LVS/DPVS/MetalLB，内核级转发，百万级并发，支持DR/NAT/TUN模式
  - L7 负载均衡：应用层，七层负载均衡，如 Nginx、OpenResty

  上面是软件负载均衡，还有硬件负载均衡，

  - 硬件负载均衡：F5、A10等

- WAF：Web应用防火墙，SQL注入/XSS/CSRF防护

- DDoS防护：高防IP/边缘WAF，流量清洗，黑洞策略，边缘拦截，分层防御-边缘节点拦截恶意流量，保护源站

### 第三层：网关层

网关层作为统一入口层，用于安全防护、流量治理、协议适配、请求路由、认证授权、限流熔断、日志监控等功能。

**API 网关**：

- Nginx： 反向代理、限流。还有在 Nginx 上加强版如下
  - OpenResty
  - kong  、 APISIX
- Spring Cloud Gateway：Java生态深度集成。
- Envoy：适合 Service Mesh 服务网格

**Kubernetes 技术环境**：

- Ingress Controller（如Nginx Ingress、Traefik）成为标准的流量入口解决方案

API 网关选型：

> 在选择 API 网关时，需要考虑性能表现、功能扩展性、运维复杂度、社区活跃度等因素。对于大规模互联网系统，建议自研或基于开源方案深度定制，以满足特定的业务需求。
>
> API 网关的设计应当遵循无状态原则，便于水平扩展，同时需要做好降级方案，确保网关故障时不影响核心业务。



**认证授权技术**：

- OAuth 2.0 / JWT / OPA。JWT无状态适合微服务，OPA细粒度策略引擎



**熔断限流**：

- Sentinel / Hystrix / Resilience4j



**协议转换技术**：

- gRPC-Web/GraphQL，HTTP/1 ↔ HTTP/2，REST ↔ gRPC



**设计实践**：

- **无状态设计**：网关不保存会话，支持水平扩展
- **插件化架构**：认证、限流、日志等功能插件化，按需加载
- **冷热分离**：核心路由规则内存缓存，配置热更新
- **灰度发布**：基于用户 ID 尾号、地域、设备类型渐进式放量
- **流量染色**：自定义网关插件。基于 Header 实现灰度、压测标记、链路隔离等

### 第四层：业务服务层



## 参考

- [《大型网站技术架构：核心原理与案例分析》](https://book.douban.com/subject/25723064/ ) 李智慧

- [《亿级流量网站架构核心技术》](https://book.douban.com/subject/26999243/) 张开涛

  